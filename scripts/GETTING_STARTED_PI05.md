# Getting Started: Ï€â‚€.â‚… Training on Vast.ai

Your complete automated training suite for fine-tuning Ï€â‚€.â‚… on `azaracla/smolvla_3dprint_plate` is ready!

## ğŸš€ TL;DR - Start Training in 30 seconds

```bash
# Set your API tokens
export HF_TOKEN="hf_YOUR_TOKEN_HERE"           # https://huggingface.co/settings/tokens
export WANDB_API_KEY="YOUR_WANDB_KEY_HERE"     # https://wandb.ai/authorize
export HF_REPO_ID="your_username/pi05_model"   # https://huggingface.co/new

# Start training
bash train_pi05_speedrun.sh
```

âœ… Done! Training will run completely automatically.

---

## ğŸ“¦ What Was Created For You

You now have a professional training suite with 8 files:

### ğŸŸ¢ **Main Scripts** (Use these to train)

| File | Purpose | How to Use |
|------|---------|-----------|
| **`train_pi05_speedrun.sh`** | Core training script | `bash train_pi05_speedrun.sh` |
| **`pi05_quick_start.sh`** | Interactive setup wizard | `bash pi05_quick_start.sh` (first time) |
| **`launch_on_vastai.sh`** | Cloud training launcher | Copy to Vast.ai, edit tokens, run |

### ğŸŸ¡ **Utilities** (Advanced usage)

| File | Purpose | How to Use |
|------|---------|-----------|
| **`train_pi05_utils.py`** | Dataset validation & config | `python train_pi05_utils.py --help` |
| **`train_pi05_resume.py`** | Checkpoint management | `python train_pi05_resume.py --help` |

### ğŸ”µ **Documentation** (Reference)

| File | Purpose | When to Read |
|------|---------|-------------|
| **`PI05_TRAINING_README.md`** | Quick reference | Want quick overview |
| **`PI05_SPEEDRUN_GUIDE.md`** | Complete documentation | Want detailed guide |
| **`pi05_speedrun_config.yaml`** | Configuration reference | Want all available options |

---

## ğŸ¯ Three Simple Paths

### **Path 1: Fully Automatic** â­ Recommended

```bash
export HF_TOKEN="hf_..."
export WANDB_API_KEY="..."
export HF_REPO_ID="username/model"
bash train_pi05_speedrun.sh
```

**What happens:**
- âœ… Validates GPU and dependencies
- âœ… Downloads dataset
- âœ… Fine-tunes Ï€â‚€.â‚… model
- âœ… Logs to Wandb in real-time
- âœ… Saves checkpoints
- âœ… Pushes final model to Hub
- âœ… Generates summary report

### **Path 2: Interactive Setup** ğŸ› ï¸ First Time?

```bash
bash pi05_quick_start.sh
```

The script will:
1. Ask for your API tokens
2. Confirm training parameters
3. Start training
4. Show results

### **Path 3: Cloud on Vast.ai** â˜ï¸ Fastest

1. **Edit `launch_on_vastai.sh`:**
   ```bash
   export HF_TOKEN="hf_YOUR_TOKEN_HERE"
   export WANDB_API_KEY="YOUR_WANDB_KEY_HERE"
   export HF_REPO_ID="your_username/pi05_model"
   ```

2. **Copy to Vast.ai GPU instance and run:**
   ```bash
   bash launch_on_vastai.sh
   ```

3. **Done!** Training runs automatically on GPU.

---

## ğŸ“Š Monitor Training

### **Option 1: Wandb Dashboard** (Real-time, Recommended)
```
https://wandb.ai/your_username/lerobot-pi05
```
- Watch loss curves live
- Track learning rate
- Monitor gradient norms
- View all metrics

### **Option 2: Local Logs**
```bash
tail -f outputs/pi05_speedrun/training.log
```

### **Option 3: Checkpoint Progress**
```bash
python train_pi05_resume.py list outputs/pi05_speedrun
```

---

## âš™ï¸ Configuration (All Optional)

Modify these **environment variables** to customize training:

```bash
# Training hyperparameters
export STEPS=3000              # How many steps (default: 3000)
export BATCH_SIZE=32           # Batch size (default: 32)
export LEARNING_RATE=1e-4      # Learning rate (default: 1e-4)

# Dataset
export DATASET_REPO="azaracla/smolvla_3dprint_plate"  # (default)

# Checkpointing
export SAVE_FREQ=500           # Save every N steps (default: 500)
export LOG_FREQ=100            # Log every N steps (default: 100)

# Resume from checkpoint
export RESUME_FROM_CHECKPOINT=true
export CHECKPOINT_PATH="outputs/pi05_speedrun/checkpoints/step_001500"
```

**Pro tip:** Default settings work great! Only change if needed.

---

## ğŸ”§ Common Tasks

### Resume Training After Interruption

```bash
# Find latest checkpoint
python train_pi05_resume.py resume outputs/pi05_speedrun

# Then run training again
bash train_pi05_speedrun.sh
```

### Save Space (Remove Old Checkpoints)

```bash
# Keep only last 3 checkpoints
python train_pi05_resume.py cleanup outputs/pi05_speedrun --keep 3
```

### Backup Important Checkpoint

```bash
python train_pi05_resume.py backup outputs/pi05_speedrun/checkpoints/step_003000 --name my_final_model
```

### Validate Dataset Before Training

```bash
python train_pi05_utils.py validate azaracla/smolvla_3dprint_plate --hf-token $HF_TOKEN
```

---

## âœ… Checklist Before Training

- [ ] Create HF token: https://huggingface.co/settings/tokens
- [ ] Create Wandb account: https://wandb.ai
- [ ] Create HF repository: https://huggingface.co/new
- [ ] Set `HF_TOKEN` environment variable
- [ ] Set `WANDB_API_KEY` environment variable
- [ ] Set `HF_REPO_ID` environment variable
- [ ] Check GPU has 20GB+ VRAM (run `nvidia-smi`)
- [ ] Verify Python 3.10+ installed
- [ ] Make scripts executable: `chmod +x train_pi05_speedrun.sh`

---

## ğŸ“ˆ Expected Results

### Training Time
- **H100 GPU:** ~1-2 hours for 3000 steps
- **A100 GPU:** ~2-3 hours for 3000 steps
- **RTX 4090:** ~4-6 hours for 3000 steps

### Cost on Vast.ai
- **Total cost:** $1-4 for full training
- **Per hour:** ~$0.5-1

### Model Performance
- Loss typically drops **30-40% after 1000 steps**
- Good performance after **3000+ steps**
- Fine-tuning is **much faster than training from scratch**

---

## ğŸ› ï¸ Troubleshooting

### GPU Out of Memory?
```bash
export BATCH_SIZE=16
bash train_pi05_speedrun.sh
```

### Dataset Access Denied?
```bash
# Verify token
python train_pi05_utils.py validate azaracla/smolvla_3dprint_plate --hf-token $HF_TOKEN

# Or re-authenticate
huggingface-cli login
```

### Training Won't Start?
```bash
# Check system info
python train_pi05_utils.py sysinfo

# Verify dependencies
python -c "import torch; import lerobot; print('OK')"
```

### More Help?
See **`PI05_SPEEDRUN_GUIDE.md`** for detailed troubleshooting (20+ solutions).

---

## ğŸš€ Quick Examples

### Example 1: Train on Local Machine (Simplest)
```bash
export HF_TOKEN="hf_abc123..."
export WANDB_API_KEY="xyz789..."
export HF_REPO_ID="myname/pi05_smolvla"
bash train_pi05_speedrun.sh
```
Takes 4-6 hours on RTX 4090, costs $0.

### Example 2: Interactive Setup
```bash
bash pi05_quick_start.sh
# Answer the prompts
# Training starts automatically
```

### Example 3: On Vast.ai (Fastest)
```bash
# 1. Edit launch_on_vastai.sh with your tokens
# 2. Copy file to Vast.ai instance
# 3. ssh into instance and run:
bash launch_on_vastai.sh
```
Takes 1-2 hours on H100, costs $2-3.

### Example 4: Custom Configuration
```bash
export STEPS=10000      # More steps for better accuracy
export BATCH_SIZE=64    # Larger batches for faster training
export LEARNING_RATE=5e-5
bash train_pi05_speedrun.sh
```

---

## ğŸ“š Documentation Map

**For Different Needs:**

1. **"How do I train right now?"** â†’ Start at the TL;DR above
2. **"I want step-by-step guide"** â†’ Read `PI05_TRAINING_README.md`
3. **"I need complete reference"** â†’ Read `PI05_SPEEDRUN_GUIDE.md`
4. **"What are all the options?"** â†’ Check `pi05_speedrun_config.yaml`
5. **"How do I use utilities?"** â†’ Run `python train_pi05_utils.py --help`

---

## ğŸ“ After Training

### View Your Results
```bash
cat outputs/pi05_speedrun/training_summary.txt
```

### Use Your Trained Model
```python
from lerobot.policies.pi05.modeling_pi05 import PI05Policy

# Load your custom trained model
model = PI05Policy.from_pretrained("your_username/pi05_smolvla")

# Use for inference
action = model.select_action(observation)
```

### Share With Others
Your model is already on HuggingFace Hub at:
```
https://huggingface.co/your_username/pi05_smolvla
```

---

## ğŸ’¾ Output Files

After training, you'll have:

```
outputs/pi05_speedrun/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ step_000500/        # Checkpoint at 500 steps
â”‚   â”œâ”€â”€ step_001000/        # Checkpoint at 1000 steps
â”‚   â””â”€â”€ step_003000/        # Final checkpoint
â”œâ”€â”€ training.log            # Complete training logs
â”œâ”€â”€ training_summary.txt    # Human-readable summary
â””â”€â”€ final/                  # Link to final model
```

**Total size:** ~50-100GB (3 checkpoints of ~15-30GB each)

---

## ğŸ¯ Next Steps

### Right Now
```bash
bash pi05_quick_start.sh
```

### Or if you know what you're doing
```bash
export HF_TOKEN="..." HF_REPO_ID="..." WANDB_API_KEY="..."
bash train_pi05_speedrun.sh
```

### Then monitor at
```
https://wandb.ai/your_username/lerobot-pi05
```

---

## â“ Common Questions

**Q: Do I need a GPU?**
A: Yes, strongly recommended. CPU training would take days.

**Q: Can I use multiple GPUs?**
A: Yes! The script auto-detects and uses all available GPUs.

**Q: What if training gets interrupted?**
A: It's resumable! Checkpoints are saved every 500 steps.

**Q: Where will the final model be?**
A: Automatically pushed to your HuggingFace Hub repo + saved locally.

**Q: How much does Vast.ai cost?**
A: ~$2-4 for full 3000-step training depending on GPU type.

**Q: Can I train longer (more steps)?**
A: Yes! Change `export STEPS=5000` before running.

---

## ğŸ†˜ Need Help?

1. **Quick issues?** Check troubleshooting above
2. **Setup questions?** Read `PI05_TRAINING_README.md`
3. **Detailed help?** See `PI05_SPEEDRUN_GUIDE.md`
4. **Still stuck?** Check training logs: `tail -f outputs/pi05_speedrun/training.log`

---

## ğŸ“‹ What This Does (Behind the Scenes)

1. **Pre-flight Check** - Verifies GPU, Python, dependencies âœ“
2. **Environment Setup** - Installs LeRobot with Pi0.5 âœ“
3. **Dataset Prep** - Downloads & validates dataset âœ“
4. **Training** - Fine-tunes Ï€â‚€.â‚… for 3000 steps âœ“
5. **Checkpointing** - Saves model every 500 steps âœ“
6. **Monitoring** - Streams metrics to Wandb âœ“
7. **Finalization** - Pushes model to Hub âœ“
8. **Reporting** - Generates summary âœ“

All **completely automatic** - you just run one command!

---

## ğŸ‰ Ready?

```bash
bash pi05_quick_start.sh
```

Or:

```bash
export HF_TOKEN="hf_..." && export WANDB_API_KEY="..." && bash train_pi05_speedrun.sh
```

**Happy training! ğŸš€**

